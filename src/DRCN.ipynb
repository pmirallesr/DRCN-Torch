{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pmirallesr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pmirallesr/eclipse-workspace/DRCN-Torch/data/\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "nbEpochs = 50\n",
    "train_batch_size = 100\n",
    "test_batch_size = 100\n",
    "source = \"SVHN\"\n",
    "target = \"MNIST\"\n",
    "sourceChannels = 1\n",
    "dataPath = os.path.dirname(os.getcwd()) + \"/data/\"\n",
    "randomSeed = 1905\n",
    "print(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f53bd3ed570>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(randomSeed)\n",
    "torch.manual_seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDenoising:\n",
    "    \"\"\"Distort a pixel with additive [+ N(0,scale)] or multiplicative [x N(1,scale)] gaussian noise\"\"\"\n",
    "\n",
    "    def __init__(self, sigma = 0.2, effectType = \"additive\"):\n",
    "        self.sigma = sigma\n",
    "        self.effectType = effectType\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if effectType == \"multiplicative\":\n",
    "            return x.numpy() * np.random.normal(loc = 1.0, scale = sigma, size = x.shape)\n",
    "        elif effectType == \"additive\":\n",
    "            return x.numpy + np.random.normal(loc = 0.0, scale = sigma, size = x.shape)\n",
    "        else:\n",
    "            print(\"Specify a valid type of gaussian error: multiplicative or additive\")\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpulseDenoising:\n",
    "    \"\"\"Erase a pixel with probability p\"\"\"\n",
    "\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.numpy() * np.random.binomial(1, self.p, size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "# Normalizations commented as torch already imports normalized data on 32x32\n",
    "\n",
    "# Data Augmentation - Geometric Transformations\n",
    "# 20ยบ random rotation\n",
    "# 20% random height and width shifts\n",
    "dataAugmentTransforms = []\n",
    "dataAugmentTransforms.append(torchvision.transforms.RandomAffine(degrees = 20, translate = (0.2, 0.2)))\n",
    "# Denoising\n",
    "dataAugmentTransforms.append(ImpulseDenoising())\n",
    "\n",
    "MNIST_Transforms = []\n",
    "MNIST_Transforms.append(torchvision.transforms.ToTensor())\n",
    "#MNIST_Transforms.append(torchvision.transforms.Resize((32,32)))\n",
    "# MNIST_Mean = 0.1307\n",
    "# MNIST_StDev = 0.3081\n",
    "# MNIST_Transforms.append(torchvision.transforms.Normalize(MNIST_Mean, MNIST_StDev))\n",
    "\n",
    "\n",
    "\n",
    "SVHN_Transforms = []\n",
    "SVHN_Transforms.append(torchvision.transforms.Grayscale())\n",
    "SVHN_Transforms.append(torchvision.transforms.ToTensor())\n",
    "# SVHN_Mean = 0.4657\n",
    "# SVHN_StDev = 0.2025\n",
    "# SVHN_Transforms.append(torchvision.transforms.Normalize(SVHN_Mean, SVHN_StDev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/SVHN/train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/SVHN/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    }
   ],
   "source": [
    "train_MNIST_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.MNIST(dataPath, \\\n",
    "                train = True, download = True, \\\n",
    "                transform = torchvision.transforms.Compose\\\n",
    "                (MNIST_Transforms + dataAugmentTransforms)), batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "test_MNIST_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.MNIST(dataPath, \\\n",
    "                train = False, download = True, \\\n",
    "                transform = torchvision.transforms.Compose\\\n",
    "                (MNIST_Transforms)), batch_size = test_batch_size, shuffle = True)\n",
    "\n",
    "#DataLoader has irregular behaviour, does not autom create an SVHN folder but does so for MNIST\n",
    "train_SVHN_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.SVHN(dataPath + \"SVHN/\", \\\n",
    "                split = \"train\", download = True, \\\n",
    "                transform = torchvision.transforms.Compose\\\n",
    "                (SVHN_Transforms)), batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "test_SVHN_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.SVHN(dataPath + \"SVHN/\", \\\n",
    "                split = \"test\", download = True , \\\n",
    "                 transform = torchvision.transforms.Compose \\\n",
    "                (SVHN_Transforms + dataAugmentTransforms)), batch_size = test_batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "# Move function to some utils module\n",
    "def calcPoolOutputSize(inputSize, kernelSize):\n",
    "    return int(inputSize/(kernelSize[0]*kernelSize[1]))\n",
    "\n",
    "       \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder common to Autoencoder and labeller\"\"\"\n",
    "\n",
    "    def __init__(self, inputChannels):\n",
    "        \"\"\"Initialize DomainRegressor.\"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #Size Parameters\n",
    "        \n",
    "        conv1OutputSize = 100\n",
    "        conv1KernelSize = 5\n",
    "        \n",
    "        maxPool1Size = (2,2)\n",
    "        \n",
    "        conv2OutputSize = 150\n",
    "        conv2KernelSize = 5\n",
    "        \n",
    "        maxPool2Size = (2,2)\n",
    "        \n",
    "        conv3OutputSize = 200\n",
    "        conv3KernelSize = 3\n",
    "        \n",
    "        # Placeholder ranges\n",
    "#         fc4OutputSize = range(300,1000,50)\n",
    "#         fc5OutputSize = range(300,1000,50)\n",
    "        fc4OutputSize = 300\n",
    "        fc5OutputSize = 300\n",
    "        \n",
    "        \n",
    "        # Size Calculations\n",
    "        conv1InputSize = inputChannels\n",
    "        \n",
    "        conv2InputSize = calcPoolOutputSize(conv1OutputSize,maxPool1Size)\n",
    "        \n",
    "        conv3InputSize = calcPoolOutputSize(conv2OutputSize,maxPool2Size)\n",
    "        \n",
    "        fc4InputSize = conv3OutputSize\n",
    "        \n",
    "        fc5InputSize = fc4OutputSize\n",
    "        \n",
    "        # Layers\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(conv1InputSize, conv1OutputSize, conv1KernelSize)\n",
    "        self.maxPool2D1 = nn.MaxPool2d(maxPool1Size)\n",
    "        self.conv2 = nn.Conv2d(conv2InputSize, conv2OutputSize, conv2KernelSize)\n",
    "        self.maxPool2D2 = nn.MaxPool2d(maxPool2Size)\n",
    "        self.conv3 = nn.Conv2d(conv3InputSize, conv3OutputSize, conv3KernelSize)\n",
    "        \n",
    "        self.fc4 = nn.Linear(fc4InputSize, fc4OutputSize)\n",
    "        self.fc5 = nn.Linear(fc5InputSize, fc5OutputSize)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass X and return probabilities of source and domain.\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxPool2D1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxPool2D2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.maxPool2D3(x)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "class Labeller(nn.Module):\n",
    "    \"\"\" The labeller part of the network is constituted by \n",
    "    the common Encoder plus a labelling fully connected layer\"\"\"\n",
    "    def __init__(self, encoder):\n",
    "        super(Labeller, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        fc3OutputSize = 10 # for 10 possible digits\n",
    "        fc3InputSize = self.encoder.fc5.out_features\n",
    "        self.fcOUT = nn.Linear(fc3InputSize, fc3OutputSize)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return F.relu(self.fcOUT(x))\n",
    "\n",
    "    \n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"The autoencoder is constituted by the Encoder common to\n",
    "    the labeller and itself, and a decoder part that is a mirror\n",
    "    image of the Encoder\n",
    "    \n",
    "    Layers 6 and 7 are FC layers, layers 8 through 10 are (de)convolutional layers\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        \"\"\"Initialize DomainRegressor.\"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        #Size Parameters\n",
    "        \n",
    "        deconv8InputSize = 100\n",
    "        deconv8KernelSize = 5\n",
    "        \n",
    "        upscale8Size = self.encoder.maxPool2D2.kernel_size\n",
    "        \n",
    "        deconv9InputSize = 150\n",
    "        deconv9KernelSize = 5\n",
    "        \n",
    "        upscale9Size = self.encoder.maxPool2D1.kernel_size\n",
    "        \n",
    "        deconv10InputSize = 200\n",
    "        deconv10KernelSize = 3\n",
    "        \n",
    "        # Placeholder ranges - substitute for actual numbers for usage\n",
    "#         fc7OutputSize = range(300,1000,50)\n",
    "#         fc6OutputSize = range(300,1000,50)\n",
    "        fc7OutputSize = 300\n",
    "        fc6OutputSize = 300\n",
    "        \n",
    "        \n",
    "        # Size Calculations\n",
    "        deconv8OutputSize = self.encoder.conv3.in_channels\n",
    "        deconv9OutputSize = self.encoder.conv2.in_channels\n",
    "        deconv10OutputSize = self.encoder.conv1.in_channels\n",
    "        \n",
    "        #Is calcPoolSize still valid for Unspooling?\n",
    "        \n",
    "#         deconv9OutputSize = calcPoolOutputSize(deconv8OutputSize,maxPool8Size)\n",
    "        \n",
    "#         deconv10OutputSize = calcPoolOutputSize(deconv9OutputSize,maxPool9Size)\n",
    "        \n",
    "        deconv8InputSize = fc7OutputSize\n",
    "        fc6InputSize = self.encoder.fc5.out_features\n",
    "        fc7InputSize = fc6OutputSize\n",
    "        \n",
    "        # Layers\n",
    "        \n",
    "        self.fc7 = nn.Linear(fc7InputSize, fc7OutputSize)\n",
    "        self.fc6 = nn.Linear(fc6InputSize, fc6OutputSize)\n",
    "        \n",
    "        self.deconv8 = nn.Conv2d(deconv8InputSize, deconv8OutputSize, deconv8KernelSize)\n",
    "        self.upsample8 = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "        self.deconv9 = nn.Conv2d(deconv9InputSize, deconv9OutputSize, deconv9KernelSize)\n",
    "        self.upsample9 = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "        self.deconv10 = nn.Conv2d(deconv10InputSize, deconv10OutputSize, deconv10KernelSize)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass X and return probabilities of source and domain.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        \n",
    "        x = F.relu(self.deconv8(x))\n",
    "        x = F.relu(self.maxUnspool2D8(x))\n",
    "        \n",
    "        x = F.relu(self.deconv9(x))\n",
    "        x = F.relu(self.maxUnspool2D9(x))\n",
    "        \n",
    "        x = self.deconv10(x)\n",
    "        \n",
    "       \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(sourceChannels)\n",
    "labeller = Labeller(encoder)\n",
    "autoencoder = Autoencoder(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controlPenalty = 0.4...0.7\n",
    "labellerOptimizer = torch.optim.RMSprop(labeller.parameters(), lr=1e-4, alpha=0.9, eps=1e-08, weight_decay=0.9, momentum=0, centered=False)\n",
    "autoencoderOptimizer = torch.optim.RMSprop(labeller.parameters(), lr=1e-4, alpha=0.9, eps=1e-08, weight_decay=0.9, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Source batch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [150, 25, 5, 5], expected input[100, 100, 14, 14] to have 25 channels, but got 100 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-41e3e7aa498e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_SVHN_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source batch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6fe6a4ceaca3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcOUT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6fe6a4ceaca3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool2D1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxPool2D2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [150, 25, 5, 5], expected input[100, 100, 14, 14] to have 25 channels, but got 100 channels instead"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "labeller.train()\n",
    "labellerOptimizer.zero_grad()\n",
    "autoencoderOptimizer.zero_grad()\n",
    "for epoch in range(nbEpochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    for batch_id, (data, target) in enumerate(train_SVHN_loader):\n",
    "        print(\"Source batch {}\".format(batch_id))\n",
    "        forward = labeller(data)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        loss = loss(forward, target)\n",
    "        tf.summary.scalar('Labeller Loss', loss, step=epoch)\n",
    "        loss.backward()\n",
    "        labellerOptimizer.step()\n",
    "        \n",
    "    for batch_id, (data, target) in enumerate(train_MNIST_loader):\n",
    "        print(\"Target batch {}\".format(batch_id))\n",
    "        forward = autoencoder(data)\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(forward, target)\n",
    "        tf.summary.scalar('Autoencoder Loss', loss, step=epoch)\n",
    "        loss.backward()\n",
    "        autoencoderOptimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
