{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pmirallesr/eclipse-workspace/DRCN-Torch/data/\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "nbEpochs = 50\n",
    "train_batch_size = 100\n",
    "test_batch_size = 100\n",
    "dropoutChance = 0.5\n",
    "lossControlPenalty = 0.4 # controlPenalty = 0.4...0.7\n",
    "source = \"SVHN\"\n",
    "target = \"MNIST\"\n",
    "sourceChannels = 1\n",
    "dataPath = os.path.dirname(os.getcwd()) + \"/data/\"\n",
    "randomSeed = 1905\n",
    "print(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f53bd3ed570>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(randomSeed)\n",
    "torch.manual_seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDenoising:\n",
    "    \"\"\"Distort a pixel with additive [+ N(0,scale)] or multiplicative [x N(1,scale)] gaussian noise\"\"\"\n",
    "\n",
    "    def __init__(self, sigma = 0.2, effectType = \"additive\"):\n",
    "        self.sigma = sigma\n",
    "        self.effectType = effectType\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if effectType == \"multiplicative\":\n",
    "            return x.numpy() * np.random.normal(loc = 1.0, scale = sigma, size = x.shape)\n",
    "        elif effectType == \"additive\":\n",
    "            return x.numpy + np.random.normal(loc = 0.0, scale = sigma, size = x.shape)\n",
    "        else:\n",
    "            print(\"Specify a valid type of gaussian error: multiplicative or additive\")\n",
    "            raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpulseDenoising:\n",
    "    \"\"\"Erase a pixel with probability p\"\"\"\n",
    "\n",
    "    def __init__(self, p = 0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x.numpy() * np.random.binomial(1, self.p, size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "# Normalizations commented as torch already imports normalized data on 32x32\n",
    "\n",
    "# Data Augmentation - Geometric Transformations\n",
    "# 20ยบ random rotation\n",
    "# 20% random height and width shifts\n",
    "dataAugmentTransforms = []\n",
    "dataAugmentTransforms.append(torchvision.transforms.RandomAffine(degrees = 20, translate = (0.2, 0.2)))\n",
    "# Denoising\n",
    "dataAugmentTransforms.append(ImpulseDenoising())\n",
    "\n",
    "MNIST_Transforms = []\n",
    "MNIST_Transforms.append(torchvision.transforms.ToTensor())\n",
    "#MNIST_Transforms.append(torchvision.transforms.Resize((32,32)))\n",
    "# MNIST_Mean = 0.1307\n",
    "# MNIST_StDev = 0.3081\n",
    "# MNIST_Transforms.append(torchvision.transforms.Normalize(MNIST_Mean, MNIST_StDev))\n",
    "\n",
    "\n",
    "\n",
    "SVHN_Transforms = []\n",
    "SVHN_Transforms.append(torchvision.transforms.Grayscale())\n",
    "SVHN_Transforms.append(torchvision.transforms.ToTensor())\n",
    "# SVHN_Mean = 0.4657\n",
    "# SVHN_StDev = 0.2025\n",
    "# SVHN_Transforms.append(torchvision.transforms.Normalize(SVHN_Mean, SVHN_StDev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: /home/pmirallesr/eclipse-workspace/DRCN-Torch/data/SVHN/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "train_MNIST_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.MNIST(dataPath, \\\n",
    "                train = True, download = True, \\\n",
    "                transform = torchvision.transforms.Compose\\\n",
    "                (MNIST_Transforms + dataAugmentTransforms)), batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "test_MNIST_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.MNIST(dataPath, \\\n",
    "                train = False, download = True, \\\n",
    "                transform = torchvision.transforms.Compose\\\n",
    "                (MNIST_Transforms)), batch_size = test_batch_size, shuffle = True)\n",
    "\n",
    "#DataLoader has irregular behaviour, does not autom create an SVHN folder but does so for MNIST\n",
    "train_SVHN_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.SVHN(dataPath + \"SVHN/\", \\\n",
    "                split = \"train\", download = True, \\\n",
    "                transform = torchvision.transforms.Compose\\\n",
    "                (SVHN_Transforms)), batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "test_SVHN_loader = torch.utils.data.DataLoader \\\n",
    "                (torchvision.datasets.SVHN(dataPath + \"SVHN/\", \\\n",
    "                split = \"test\", download = True , \\\n",
    "                 transform = torchvision.transforms.Compose \\\n",
    "                (SVHN_Transforms + dataAugmentTransforms)), batch_size = test_batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcConvOutputDimensions(inputDim, kernelSize, padding = (0,0), dilation = (1,1), stride = (1,1)):\n",
    "    variables = [inputDim, kernelSize, padding, dilation, stride]\n",
    "    for i, elem in enumerate(variables):\n",
    "        if isinstance(elem, int):\n",
    "            variables[i] = (elem, elem)\n",
    "        elif isinstance(elem, tuple):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError\n",
    "    Hout = math.floor((variables[0][0] + 2*variables[2][0] - variables[3][0]*(variables[1][0] - 1) -1)/variables[4][0] + 1)\n",
    "    Wout = math.floor((variables[0][1] + 2*variables[2][1] - variables[3][1]*(variables[1][1] - 1) -1)/variables[4][1] + 1)\n",
    "    return (Hout, Wout)\n",
    "\n",
    "def calcPoolOutputDimensions(inputDim, pooling):\n",
    "    variables = [inputDim, pooling]\n",
    "    for i, elem in enumerate(variables):\n",
    "        if isinstance(elem, int):\n",
    "            variables[i] = (elem, elem)\n",
    "        elif isinstance(elem, tuple):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError\n",
    "    return tuple(int(inputDim[i]/pooling[i]) for i,ti in enumerate(pooling) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder common to Autoencoder and labeller\"\"\"\n",
    "\n",
    "    def __init__(self, inputChannels, dropoutChance = 0.5):\n",
    "        \"\"\"Initialize DomainRegressor.\"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #Size Parameters\n",
    "        \n",
    "        conv1Filters = 100\n",
    "        conv1KernelSize = 5\n",
    "        \n",
    "        maxPool1Size = (2,2)\n",
    "        \n",
    "        conv2Filters = 150\n",
    "        conv2KernelSize = 5\n",
    "        \n",
    "        maxPool2Size = (2,2)\n",
    "        \n",
    "        conv3Filters = 200\n",
    "        conv3KernelSize = 3\n",
    "        \n",
    "        # Placeholder ranges\n",
    "#         fc4OutputSize = range(300,1000,50)\n",
    "#         fc5OutputSize = range(300,1000,50)\n",
    "        fc4OutputDim = 300\n",
    "        fc5OutputDim = 300\n",
    "        \n",
    "        \n",
    "        # Convolutional Layers Size Calculations\n",
    "        conv1InputChannels = inputChannels\n",
    "        conv2InputChannels = conv1Filters\n",
    "        conv3InputChannels = conv2Filters\n",
    "                \n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(conv1InputChannels, conv1Filters, conv1KernelSize)\n",
    "        self.maxPool2D1 = nn.MaxPool2d(maxPool1Size)\n",
    "        self.conv2 = nn.Conv2d(conv2InputChannels, conv2Filters, conv2KernelSize)\n",
    "        self.maxPool2D2 = nn.MaxPool2d(maxPool2Size)\n",
    "        self.conv3 = nn.Conv2d(conv3InputChannels, conv3Filters, conv3KernelSize)\n",
    "        \n",
    "        # Fully Connected Layers Size Calculations\n",
    "        conv1OutputDim = calcConvOutputDimensions(32, conv1KernelSize) #Size of SVHN images!\n",
    "        conv2InputDim = calcPoolOutputDimensions(conv1OutputDim, maxPool1Size)\n",
    "        conv2OutputDim = calcConvOutputDimensions(conv2InputDim, conv2KernelSize)\n",
    "        conv3InputDim = calcPoolOutputDimensions(conv2OutputDim, maxPool2Size)\n",
    "        conv3OutputDim = calcConvOutputDimensions(conv3InputDim, conv3KernelSize)\n",
    "        fc4InputDim = conv3Filters*conv3OutputDim[0]*conv3OutputDim[1]\n",
    "        fc5InputDim = fc4OutputDim\n",
    "        \n",
    "        # Fully connected Layers\n",
    "        self.fc4 = nn.Linear(fc4InputDim, fc4OutputDim)\n",
    "        self.dropout4 = nn.Dropout(p = dropoutChance)\n",
    "        \n",
    "        self.fc5 = nn.Linear(fc5InputDim, fc5OutputDim)\n",
    "        self.dropout5 = nn.Dropout(p = dropoutChance)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass X and return probabilities of source and domain.\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxPool2D1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxPool2D2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labeller(nn.Module):\n",
    "    \"\"\" The labeller part of the network is constituted by \n",
    "    the common Encoder plus a labelling fully connected layer\"\"\"\n",
    "    def __init__(self, encoder):\n",
    "        super(Labeller, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        fc3OutputSize = 10 # for 10 possible digits\n",
    "        fc3InputSize = self.encoder.fc5.out_features\n",
    "        self.fcOUT = nn.Linear(fc3InputSize, fc3OutputSize)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return F.relu(self.fcOUT(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"The autoencoder is constituted by the Encoder common to\n",
    "    the labeller and itself, and a decoder part that is a mirror\n",
    "    image of the Encoder\n",
    "    \n",
    "    Layers 6 and 7 are FC layers, layers 8 through 10 are (de)convolutional layers\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        \"\"\"Initialize DomainRegressor.\"\"\"\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        #Size Parameters\n",
    "        \n",
    "        deconv8InputSize = 100\n",
    "        deconv8KernelSize = 5\n",
    "        \n",
    "        upscale8Size = self.encoder.maxPool2D2.kernel_size\n",
    "        \n",
    "        deconv9InputSize = 150\n",
    "        deconv9KernelSize = 5\n",
    "        \n",
    "        upscale9Size = self.encoder.maxPool2D1.kernel_size\n",
    "        \n",
    "        deconv10InputSize = 200\n",
    "        deconv10KernelSize = 3\n",
    "        \n",
    "        # Placeholder ranges - substitute for actual numbers for usage\n",
    "#         fc7OutputSize = range(300,1000,50)\n",
    "#         fc6OutputSize = range(300,1000,50)\n",
    "        fc7OutputSize = 300\n",
    "        fc6OutputSize = 300\n",
    "        \n",
    "        \n",
    "        # Size Calculations\n",
    "        deconv8OutputSize = self.encoder.conv3.in_channels\n",
    "        deconv9OutputSize = self.encoder.conv2.in_channels\n",
    "        deconv10OutputSize = self.encoder.conv1.in_channels\n",
    "        \n",
    "        #Is calcPoolSize still valid for Unspooling?\n",
    "        \n",
    "#         deconv9OutputSize = calcPoolOutputSize(deconv8OutputSize,maxPool8Size)\n",
    "        \n",
    "#         deconv10OutputSize = calcPoolOutputSize(deconv9OutputSize,maxPool9Size)\n",
    "        \n",
    "        deconv8InputSize = fc7OutputSize\n",
    "        fc6InputSize = self.encoder.fc5.out_features\n",
    "        fc7InputSize = fc6OutputSize\n",
    "        \n",
    "        # Layers\n",
    "        \n",
    "        self.fc7 = nn.Linear(fc7InputSize, fc7OutputSize)\n",
    "        self.fc6 = nn.Linear(fc6InputSize, fc6OutputSize)\n",
    "        \n",
    "        self.deconv8 = nn.Conv2d(deconv8InputSize, deconv8OutputSize, deconv8KernelSize)\n",
    "        self.upsample8 = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "        self.deconv9 = nn.Conv2d(deconv9InputSize, deconv9OutputSize, deconv9KernelSize)\n",
    "        self.upsample9 = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "        self.deconv10 = nn.Conv2d(deconv10InputSize, deconv10OutputSize, deconv10KernelSize)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass X and return probabilities of source and domain.\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        \n",
    "        x = F.relu(self.deconv8(x))\n",
    "        x = F.relu(self.maxUnspool2D8(x))\n",
    "        \n",
    "        x = F.relu(self.deconv9(x))\n",
    "        x = F.relu(self.maxUnspool2D9(x))\n",
    "        \n",
    "        x = self.deconv10(x)\n",
    "        \n",
    "       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(sourceChannels)\n",
    "labeller = Labeller(encoder)\n",
    "autoencoder = Autoencoder(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(list(labeller.parameters()) + list(autoencoder.parameters()), lr=1e-4, alpha=0.9, eps=1e-08, weight_decay=0.9, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch 1:0. Loss = 0.920518\n",
      "Source batch 1:1. Loss = 0.920098\n",
      "Source batch 1:2. Loss = 0.921502\n",
      "Source batch 1:3. Loss = 0.920841\n",
      "Source batch 1:4. Loss = 0.921345\n",
      "Source batch 1:5. Loss = 0.920630\n",
      "Source batch 1:6. Loss = 0.921894\n",
      "Source batch 1:7. Loss = 0.921284\n",
      "Source batch 1:8. Loss = 0.920780\n",
      "Source batch 1:9. Loss = 0.921992\n",
      "Source batch 1:10. Loss = 0.921837\n",
      "Source batch 1:11. Loss = 0.920582\n",
      "Source batch 1:12. Loss = 0.921401\n",
      "Source batch 1:13. Loss = 0.920938\n",
      "Source batch 1:14. Loss = 0.920776\n",
      "Source batch 1:15. Loss = 0.919518\n",
      "Source batch 1:16. Loss = 0.920362\n",
      "Source batch 1:17. Loss = 0.920582\n",
      "Source batch 1:18. Loss = 0.920827\n",
      "Source batch 1:19. Loss = 0.922265\n",
      "Source batch 1:20. Loss = 0.922041\n",
      "Source batch 1:21. Loss = 0.920888\n",
      "Source batch 1:22. Loss = 0.920321\n",
      "Source batch 1:23. Loss = 0.922328\n",
      "Source batch 1:24. Loss = 0.921375\n",
      "Source batch 1:25. Loss = 0.920510\n",
      "Source batch 1:26. Loss = 0.920531\n",
      "Source batch 1:27. Loss = 0.921483\n",
      "Source batch 1:28. Loss = 0.921333\n",
      "Source batch 1:29. Loss = 0.920257\n",
      "Source batch 1:30. Loss = 0.921974\n",
      "Source batch 1:31. Loss = 0.920578\n",
      "Source batch 1:32. Loss = 0.920730\n",
      "Source batch 1:33. Loss = 0.921111\n",
      "Source batch 1:34. Loss = 0.921367\n",
      "Source batch 1:35. Loss = 0.920536\n",
      "Source batch 1:36. Loss = 0.921290\n",
      "Source batch 1:37. Loss = 0.920904\n",
      "Source batch 1:38. Loss = 0.921225\n",
      "Source batch 1:39. Loss = 0.921078\n",
      "Source batch 1:40. Loss = 0.920905\n",
      "Source batch 1:41. Loss = 0.921442\n",
      "Source batch 1:42. Loss = 0.921243\n",
      "Source batch 1:43. Loss = 0.920222\n",
      "Source batch 1:44. Loss = 0.921720\n",
      "Source batch 1:45. Loss = 0.919008\n",
      "Source batch 1:46. Loss = 0.920759\n",
      "Source batch 1:47. Loss = 0.920999\n",
      "Source batch 1:48. Loss = 0.922346\n",
      "Source batch 1:49. Loss = 0.922297\n",
      "Source batch 1:50. Loss = 0.920952\n",
      "Source batch 1:51. Loss = 0.920646\n",
      "Source batch 1:52. Loss = 0.920044\n",
      "Source batch 1:53. Loss = 0.921301\n",
      "Source batch 1:54. Loss = 0.920757\n",
      "Source batch 1:55. Loss = 0.920625\n",
      "Source batch 1:56. Loss = 0.920341\n",
      "Source batch 1:57. Loss = 0.920215\n",
      "Source batch 1:58. Loss = 0.920212\n",
      "Source batch 1:59. Loss = 0.920736\n",
      "Source batch 1:60. Loss = 0.921039\n",
      "Source batch 1:61. Loss = 0.921058\n",
      "Source batch 1:62. Loss = 0.920906\n",
      "Source batch 1:63. Loss = 0.920851\n",
      "Source batch 1:64. Loss = 0.920425\n",
      "Source batch 1:65. Loss = 0.923024\n",
      "Source batch 1:66. Loss = 0.920732\n",
      "Source batch 1:67. Loss = 0.918815\n",
      "Source batch 1:68. Loss = 0.920665\n",
      "Source batch 1:69. Loss = 0.921087\n",
      "Source batch 1:70. Loss = 0.920652\n",
      "Source batch 1:71. Loss = 0.921421\n",
      "Source batch 1:72. Loss = 0.921410\n",
      "Source batch 1:73. Loss = 0.921497\n",
      "Source batch 1:74. Loss = 0.921605\n",
      "Source batch 1:75. Loss = 0.919842\n",
      "Source batch 1:76. Loss = 0.921328\n",
      "Source batch 1:77. Loss = 0.921163\n",
      "Source batch 1:78. Loss = 0.920522\n",
      "Source batch 1:79. Loss = 0.920861\n",
      "Source batch 1:80. Loss = 0.919595\n",
      "Source batch 1:81. Loss = 0.920475\n",
      "Source batch 1:82. Loss = 0.921613\n",
      "Source batch 1:83. Loss = 0.921697\n",
      "Source batch 1:84. Loss = 0.919440\n",
      "Source batch 1:85. Loss = 0.920427\n",
      "Source batch 1:86. Loss = 0.920251\n",
      "Source batch 1:87. Loss = 0.920879\n",
      "Source batch 1:88. Loss = 0.921680\n",
      "Source batch 1:89. Loss = 0.921054\n",
      "Source batch 1:90. Loss = 0.922450\n",
      "Source batch 1:91. Loss = 0.922509\n",
      "Source batch 1:92. Loss = 0.921563\n",
      "Source batch 1:93. Loss = 0.920970\n",
      "Source batch 1:94. Loss = 0.919676\n",
      "Source batch 1:95. Loss = 0.920712\n",
      "Source batch 1:96. Loss = 0.921201\n",
      "Source batch 1:97. Loss = 0.921055\n",
      "Source batch 1:98. Loss = 0.922367\n",
      "Source batch 1:99. Loss = 0.919734\n",
      "Source batch 1:100. Loss = 0.922404\n",
      "Source batch 1:101. Loss = 0.921976\n",
      "Source batch 1:102. Loss = 0.920228\n",
      "Source batch 1:103. Loss = 0.919679\n",
      "Source batch 1:104. Loss = 0.920907\n",
      "Source batch 1:105. Loss = 0.919947\n",
      "Source batch 1:106. Loss = 0.920968\n",
      "Source batch 1:107. Loss = 0.919253\n",
      "Source batch 1:108. Loss = 0.921490\n",
      "Source batch 1:109. Loss = 0.921059\n",
      "Source batch 1:110. Loss = 0.920696\n",
      "Source batch 1:111. Loss = 0.922417\n",
      "Source batch 1:112. Loss = 0.921320\n",
      "Source batch 1:113. Loss = 0.922131\n",
      "Source batch 1:114. Loss = 0.920945\n",
      "Source batch 1:115. Loss = 0.922596\n",
      "Source batch 1:116. Loss = 0.920951\n",
      "Source batch 1:117. Loss = 0.921054\n",
      "Source batch 1:118. Loss = 0.920726\n",
      "Source batch 1:119. Loss = 0.921708\n",
      "Source batch 1:120. Loss = 0.921977\n",
      "Source batch 1:121. Loss = 0.920998\n",
      "Source batch 1:122. Loss = 0.922701\n",
      "Source batch 1:123. Loss = 0.920889\n",
      "Source batch 1:124. Loss = 0.922107\n",
      "Source batch 1:125. Loss = 0.922449\n",
      "Source batch 1:126. Loss = 0.921423\n",
      "Source batch 1:127. Loss = 0.921569\n",
      "Source batch 1:128. Loss = 0.921248\n",
      "Source batch 1:129. Loss = 0.922173\n",
      "Source batch 1:130. Loss = 0.921012\n",
      "Source batch 1:131. Loss = 0.921186\n",
      "Source batch 1:132. Loss = 0.920555\n",
      "Source batch 1:133. Loss = 0.919555\n",
      "Source batch 1:134. Loss = 0.921346\n",
      "Source batch 1:135. Loss = 0.922678\n",
      "Source batch 1:136. Loss = 0.921979\n",
      "Source batch 1:137. Loss = 0.919136\n",
      "Source batch 1:138. Loss = 0.920149\n",
      "Source batch 1:139. Loss = 0.921896\n",
      "Source batch 1:140. Loss = 0.921655\n",
      "Source batch 1:141. Loss = 0.921234\n",
      "Source batch 1:142. Loss = 0.920408\n",
      "Source batch 1:143. Loss = 0.922087\n",
      "Source batch 1:144. Loss = 0.921367\n",
      "Source batch 1:145. Loss = 0.920696\n",
      "Source batch 1:146. Loss = 0.921043\n",
      "Source batch 1:147. Loss = 0.920810\n",
      "Source batch 1:148. Loss = 0.921496\n",
      "Source batch 1:149. Loss = 0.921467\n",
      "Source batch 1:150. Loss = 0.921670\n",
      "Source batch 1:151. Loss = 0.921994\n",
      "Source batch 1:152. Loss = 0.921508\n",
      "Source batch 1:153. Loss = 0.922755\n",
      "Source batch 1:154. Loss = 0.920498\n",
      "Source batch 1:155. Loss = 0.921753\n",
      "Source batch 1:156. Loss = 0.922109\n",
      "Source batch 1:157. Loss = 0.921050\n",
      "Source batch 1:158. Loss = 0.921484\n",
      "Source batch 1:159. Loss = 0.920755\n",
      "Source batch 1:160. Loss = 0.921499\n",
      "Source batch 1:161. Loss = 0.920427\n",
      "Source batch 1:162. Loss = 0.921009\n",
      "Source batch 1:163. Loss = 0.921425\n",
      "Source batch 1:164. Loss = 0.920594\n",
      "Source batch 1:165. Loss = 0.921006\n",
      "Source batch 1:166. Loss = 0.920720\n",
      "Source batch 1:167. Loss = 0.921912\n",
      "Source batch 1:168. Loss = 0.922557\n",
      "Source batch 1:169. Loss = 0.921307\n",
      "Source batch 1:170. Loss = 0.920605\n",
      "Source batch 1:171. Loss = 0.922109\n",
      "Source batch 1:172. Loss = 0.921420\n",
      "Source batch 1:173. Loss = 0.921406\n",
      "Source batch 1:174. Loss = 0.922491\n",
      "Source batch 1:175. Loss = 0.920576\n",
      "Source batch 1:176. Loss = 0.919272\n",
      "Source batch 1:177. Loss = 0.921355\n",
      "Source batch 1:178. Loss = 0.919436\n",
      "Source batch 1:179. Loss = 0.919445\n",
      "Source batch 1:180. Loss = 0.918927\n",
      "Source batch 1:181. Loss = 0.920500\n",
      "Source batch 1:182. Loss = 0.918926\n",
      "Source batch 1:183. Loss = 0.920212\n",
      "Source batch 1:184. Loss = 0.922048\n",
      "Source batch 1:185. Loss = 0.920735\n",
      "Source batch 1:186. Loss = 0.921520\n",
      "Source batch 1:187. Loss = 0.921421\n",
      "Source batch 1:188. Loss = 0.921107\n",
      "Source batch 1:189. Loss = 0.920845\n",
      "Source batch 1:190. Loss = 0.920152\n",
      "Source batch 1:191. Loss = 0.919785\n",
      "Source batch 1:192. Loss = 0.921427\n",
      "Source batch 1:193. Loss = 0.919527\n",
      "Source batch 1:194. Loss = 0.919525\n",
      "Source batch 1:195. Loss = 0.921673\n",
      "Source batch 1:196. Loss = 0.921465\n",
      "Source batch 1:197. Loss = 0.919021\n",
      "Source batch 1:198. Loss = 0.921211\n",
      "Source batch 1:199. Loss = 0.921286\n",
      "Source batch 1:200. Loss = 0.920358\n",
      "Source batch 1:201. Loss = 0.920109\n",
      "Source batch 1:202. Loss = 0.920876\n",
      "Source batch 1:203. Loss = 0.920212\n",
      "Source batch 1:204. Loss = 0.921553\n",
      "Source batch 1:205. Loss = 0.920084\n",
      "Source batch 1:206. Loss = 0.920040\n",
      "Source batch 1:207. Loss = 0.922034\n",
      "Source batch 1:208. Loss = 0.921394\n",
      "Source batch 1:209. Loss = 0.920199\n",
      "Source batch 1:210. Loss = 0.920626\n",
      "Source batch 1:211. Loss = 0.921949\n",
      "Source batch 1:212. Loss = 0.920695\n",
      "Source batch 1:213. Loss = 0.921062\n",
      "Source batch 1:214. Loss = 0.922042\n",
      "Source batch 1:215. Loss = 0.920176\n",
      "Source batch 1:216. Loss = 0.922225\n",
      "Source batch 1:217. Loss = 0.919605\n",
      "Source batch 1:218. Loss = 0.920926\n",
      "Source batch 1:219. Loss = 0.921112\n",
      "Source batch 1:220. Loss = 0.921210\n",
      "Source batch 1:221. Loss = 0.920827\n",
      "Source batch 1:222. Loss = 0.919863\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "labeller.train()\n",
    "optimizer.zero_grad()\n",
    "start = time.time()\n",
    "for epoch in range(1, nbEpochs + 1):\n",
    "    epochStart = time.time()\n",
    "    for batch_id, (data, target) in enumerate(train_SVHN_loader):\n",
    "        forward = labeller(data)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        loss = lossControlPenalty * loss(forward, target)\n",
    "#         tf.summary.scalar('Labeller_Loss', loss.detach(), epoch)\n",
    "        print(\"Source batch {}:{}. Loss = {:5f}\".format(epoch, batch_id, loss))\n",
    "        loss.backward()\n",
    "        labellerOptimizer.step()\n",
    "        \n",
    "    for batch_id, (data, target) in enumerate(train_MNIST_loader):\n",
    "        print(\"Target batch {}:{}. Loss = {:5f}\".format(epoch, batch_id, loss))\n",
    "        forward = autoencoder(data)\n",
    "        loss = nn.MSELoss()\n",
    "        loss = (1 - lossControlPenalty) * loss(forward, target)\n",
    "#         tf.summary.scalar('Autoencoder_Loss', loss.detach(), epoch)\n",
    "        loss.backward()\n",
    "        autoencoderOptimizer.step()\n",
    "    epochEnd = time.time()\n",
    "    print(\"------------\\nEpoch {} completed in {:5f}\\n{} s elapsed\\n--------------\".format(epoch, epochEnd - epochStart, epochEnd - start))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
